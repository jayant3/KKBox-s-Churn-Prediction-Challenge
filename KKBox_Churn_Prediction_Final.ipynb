{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to get median values for all numerical features\n",
    "def get_median(data,numerical_columns):\n",
    "    \n",
    "    median_dict = dict()\n",
    "    \n",
    "    for column in numerical_columns:\n",
    "        \n",
    "        median_dict[column] = np.nanmedian(data[column])\n",
    "        \n",
    "    return median_dict\n",
    "\n",
    "#=================================================================\n",
    "\n",
    "\n",
    "# function to get mode values for all categorical features\n",
    "def get_mode(data,categorical_columns):\n",
    "    \n",
    "    mode_dict = dict()\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        \n",
    "        mode_dict[column] = data[column].value_counts().index[0]\n",
    "        \n",
    "    return mode_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hjsdssz/kkbox-churn-prediction-model \n",
    "\n",
    "# function to fill NULL values of numerical features with mean\n",
    "\n",
    "def fill_with_median(data,numerical_columns,train_data_median):\n",
    "    \n",
    "    for column in numerical_columns:\n",
    "        data[column].fillna(train_data_median[column],inplace=True)\n",
    "\n",
    "#===============================================================================\n",
    "\n",
    "# function to fill NULL values of categorical features with most occuring values \n",
    "\n",
    "def fill_with_mode(data,categorical_columns,train_data_mode): \n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        data[column].fillna(train_data_mode[column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_parameters(x,y):\n",
    "    \n",
    "    '''\n",
    "    function to identify best parameters for LGBM Classifier using RandomizedSearchCV\n",
    "    '''\n",
    "    \n",
    "    lgb = LGBMClassifier(n_jobs=-1)\n",
    "\n",
    "    param = {\n",
    "               'max_depth':[2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "               'learning_rate':[0.01,0.05,0.1,0.15,0.2,0.3,0.5,0.8,1],\n",
    "               'subsample':[0.2,0.3,0.5,0.8,0.9,1],\n",
    "               'colsample_bytree':[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "               'n_estimators':[50,100,150,200,250,300,400,500,700,800],\n",
    "               'num_leaves':[31,35,40,45,50,55,60,70,80,90,100,200]\n",
    "               }\n",
    "\n",
    "    random_clf = RandomizedSearchCV(lgb, param_distributions=param, cv=10, verbose=1, n_jobs=-1)\n",
    "\n",
    "    random_clf.fit(x,y)\n",
    "\n",
    "    best_param = random_clf.best_params_\n",
    "    \n",
    "    return best_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(col):\n",
    "    '''\n",
    "    function for one-hot encoding of class-label y\n",
    "    '''\n",
    "    col = list(col)\n",
    "    arr = np.zeros((len(col),2))\n",
    "    for i,val in enumerate(col):\n",
    "        arr[i,val] = 1\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    '''\n",
    "    function to train model for predicting customer churn\n",
    "    '''\n",
    "    #reading data from csv files\n",
    "    trainv2 = pd.read_csv(\"input_files/train_v2.csv\",dtype={'is_churn':np.int8})\n",
    "    \n",
    "    #sample_submission_v2 = pd.read_csv(\"input_files/sample_submission_v2.csv\",dtype={'is_churn':np.int8})\n",
    "    \n",
    "    transactions_v2 = pd.read_csv(\"input_files/transactions_v2.csv\",\n",
    "                              dtype={'payment_method_id':np.int8,'payment_plan_days':np.int16,'plan_list_price':np.int16,\n",
    "                                    'actual_amount_paid':np.int16,'is_auto_renew':np.int8,'is_cancel':np.int8})\n",
    "    members = pd.read_csv(\"input_files/members_v3.csv\",\n",
    "                     dtype={'city':np.int8,'bd':np.int16,'registered_via':np.int8})\n",
    "    user_logs_v2 = pd.read_csv(\"input_files/user_logs_v2.csv\",\n",
    "                     dtype={'num_25': np.int16,'num_50': np.int16,'num_75': np.int16,'num_985': np.int16,'num_100': np.int16})\n",
    "    \n",
    "    # taking transaction count per user\n",
    "    transaction_count_per_user = transactions_v2.groupby(\"msno\")['msno'].count()\n",
    "    \n",
    "    # converting transaction count to dataframe\n",
    "    transaction_count_per_user = pd.DataFrame(transaction_count_per_user)\n",
    "    transaction_count_per_user.columns = ['trans_count']\n",
    "    transaction_count_per_user.reset_index(inplace=True)\n",
    "    \n",
    "    #keeping only latest transaction record for each user\n",
    "    transactions_v2 = transactions_v2.drop_duplicates(subset=['msno'], keep='first')\n",
    "    #dropping columns\n",
    "    transactions_v2.drop(columns=['membership_expire_date','actual_amount_paid','is_cancel'],axis=1,inplace=True)\n",
    "    #merging transaction count to dataframe\n",
    "    transactions_v2 = transactions_v2.merge(transaction_count_per_user, how='left',on='msno')\n",
    "    \n",
    "    user_logs_v2.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # removing datapoints with negative value of num_100\n",
    "    user_logs_v2.drop(index=(user_logs_v2[user_logs_v2.num_100<0].index) ,inplace=True)\n",
    "    \n",
    "    # converting total seconds to total minutes by dividing it by 60\n",
    "    user_logs_v2['total_minutes'] = user_logs_v2['total_secs']/60\n",
    "    user_logs_v2.drop(columns=['total_secs'],inplace=True)  # dropping column 'total_secs'\n",
    "    \n",
    "    # taking sum of each columns for each user\n",
    "    user_logs_sum = user_logs_v2.groupby('msno',as_index=False).sum()\n",
    "    user_logs_sum.drop(columns=['date'],axis=1,inplace=True)\n",
    "    \n",
    "    # taking mean of each columns for each user\n",
    "    user_logs_mean = user_logs_v2.groupby('msno',as_index=False).mean()\n",
    "    user_logs_mean.columns = ['msno','date','mean_num_25','mean_num_50','mean_num_75','mean_num_985','mean_num_100','mean_num_unq','mean_total_min']\n",
    "    user_logs_mean.drop(columns=['date'],axis=1,inplace=True)\n",
    "    \n",
    "    # merging dataframe\n",
    "    user_logs = user_logs_sum.merge(user_logs_mean,on='msno')\n",
    "    \n",
    "    members.drop_duplicates(inplace=True)\n",
    "    \n",
    "    train = trainv2.merge(transactions_v2,how='left',on='msno')\n",
    "    members.drop(columns=['gender'],axis=1,inplace=True)  # dropping 'gender' column as 65% values are NULL\n",
    "    train = train.merge(members,how='left',on='msno')  \n",
    "    train = train.merge(user_logs,how='left',on='msno')\n",
    "    print(\"Shape after merging all dataframes = \",train.shape)\n",
    "    \n",
    "    numerical_columns = ['payment_plan_days','plan_list_price','num_25', 'num_50', 'num_75', 'num_985', 'num_100',\n",
    "                     'num_unq', 'total_minutes', 'mean_num_25', 'mean_num_50', 'mean_num_75','mean_num_985', \n",
    "                     'mean_num_100', 'mean_num_unq', 'mean_total_min','bd','trans_count']\n",
    "    \n",
    "    categorical_columns = ['payment_method_id','is_auto_renew','city','registered_via','transaction_date','registration_init_time']\n",
    "    \n",
    "    # getting mdian and mode values from train data\n",
    "    train_data_median = get_median(train,numerical_columns)\n",
    "    train_data_mode = get_mode(train,categorical_columns)\n",
    "    \n",
    "    # filling missing numerical values with median\n",
    "    fill_with_median(train,numerical_columns,train_data_median)\n",
    "\n",
    "    # filling missing categorical values and missing date with mode\n",
    "    fill_with_mode(train,categorical_columns,train_data_mode)\n",
    "    \n",
    "    train.drop(columns='bd',axis=1,inplace=True)\n",
    "    \n",
    "    # creating new features out of transaction date \n",
    "\n",
    "    # taking just day of month\n",
    "    train['trans_day_of_month'] = train.transaction_date.apply(lambda x: np.int8(str(int(x))[6:]))\n",
    "\n",
    "    # taking just month of transaction\n",
    "    #train['trans_month'] = train.transaction_date.apply(lambda x: np.int8(str(int(x))[4:6]) )\n",
    "\n",
    "    # taking removing year from the date and taking just month and day of month \n",
    "    train['trans_month_day'] = train.transaction_date.apply(lambda x: np.int16(str(int(x))[4:]))\n",
    "\n",
    "    # 'day_of_week' signifies which day of week it is. \n",
    "    # e.g.  Monday is 1 and Sunday is 7\n",
    "    train['trans_day_of_week'] = train.transaction_date.apply(lambda x: datetime.strptime(str(int(x)), \"%Y%m%d\").isoweekday())\n",
    "\n",
    "    #=========================================================================================================\n",
    "\n",
    "    # creating new features out of registration date \n",
    "\n",
    "    # taking just day of month\n",
    "    train['reg_day_of_month'] = train.registration_init_time.apply(lambda x: np.int8(str(int(x))[6:]))\n",
    "\n",
    "    # taking just month of transaction\n",
    "    train['reg_month'] = train.registration_init_time.apply(lambda x: np.int8(str(int(x))[4:6]) )\n",
    "\n",
    "    # taking removing year from the date and taking just month and day of month \n",
    "    train['reg_month_day'] = train.registration_init_time.apply(lambda x: np.int16(str(int(x))[4:]))\n",
    "\n",
    "    # 'day_of_week' signifies which day of week it is. \n",
    "    # e.g.  Monday is 1 and Sunday is 7\n",
    "    train['reg_day_of_week'] = train.registration_init_time.apply(lambda x: datetime.strptime(str(int(x)), \"%Y%m%d\").isoweekday())\n",
    "    \n",
    "    #==========================================================================================================\n",
    "    \n",
    "    # creating a new feature - price_per_day\n",
    "    train['price_per_day'] = [ (train['plan_list_price'][i]/train['payment_plan_days'][i]) if (train['payment_plan_days'][i]!=0) else 0 for i in range(train.shape[0]) ]\n",
    "\n",
    "    #=====================================================================================================\n",
    "    \n",
    "    # creating a new feature - min_per_song i.e. average minutes listened per song\n",
    "    train['min_per_song'] = train['total_minutes']/(train['num_25']+train['num_50']+train['num_75']+train['num_985']+train['num_100'])\n",
    "\n",
    "    # creating a new feature - avg_min_per_unq i.e. average minutes listened per unique song\n",
    "    train['avg_min_per_unq'] = train['mean_total_min']/train['mean_num_unq']\n",
    "\n",
    "    #========================================================================================================\n",
    "\n",
    "    # creating a feature 'total_num' i.e. total number of songs listened by the user\n",
    "    train['total_num'] = train['num_25']+train['num_50']+train['num_75']+train['num_985']+train['num_100']\n",
    "\n",
    "    # creating a feature 'ratio_num_25' i.e. ratio of 'total_num'(total number of songs) and 'num_25'(number of songs listened upto 25% of their length)\n",
    "    # train['ratio_num_25'] = [train['total_num'][i]/train['num_25'][i] for i in range(train.shape[0])]\n",
    "    train['ratio_num_25'] = train['num_25']/train['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_50' i.e. ratio of 'total_num'(total number of songs) and 'num_50'(number of songs listened upto 50% of their length)\n",
    "    train['ratio_num_50'] = train['num_50']/train['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_75' i.e. ratio of 'total_num'(total number of songs) and 'num_75'(number of songs listened upto 75% of their length)\n",
    "    train['ratio_num_75'] = train['num_75']/train['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_985' i.e. ratio of 'total_num'(total number of songs) and 'num_985'(number of songs listened upto 98.5% of their length)\n",
    "    train['ratio_num_985'] = train['num_985']/train['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_100' i.e. ratio of 'total_num'(total number of songs) and 'num_100'(number of songs listened upto 100% of their length)\n",
    "    train['ratio_num_100'] = train['num_100']/train['total_num']\n",
    "\n",
    "    # creating a feature 'total_by_unq' i.e. total_num divided by num_unq\n",
    "    train['total_by_unq'] = train['total_num']/train['num_unq']\n",
    "    \n",
    "    print(\"Shape after feature engineering = \",train.shape)\n",
    "    #===========================================================================================================\n",
    "    \n",
    "    #splitting data in to train and test set \n",
    "    X = train.drop(columns=['msno','is_churn','transaction_date','registration_init_time']).reset_index(drop=True)\n",
    "    Y = train['is_churn']\n",
    "    \n",
    "    # splitting into train test ratio of 75:25 i.e. 75% data for training and 25% for testing\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.25,stratify=Y)\n",
    "    \n",
    "    print(\"Shape after splitting:\")\n",
    "    print(\"Train shape: x_train = \",x_train.shape,\", y_train = \",y_train.shape)\n",
    "    print(\"Test shape: x_test = \",x_test.shape,\", y_test = \",y_test.shape)\n",
    "\n",
    "    \n",
    "    #============================================================================================================\n",
    "    \n",
    "    # training model\n",
    "    \n",
    "    # LightGBM 1\n",
    "    \n",
    "    print(\"LightGBM 1\")\n",
    "    start_1 = datetime.now()\n",
    "    \n",
    "    # getting best parameters \n",
    "    best_param_1 = get_best_parameters(x_train,y_train)\n",
    "\n",
    "    print(\"LGBMClassifier-1 best parameters:\", best_param_1)\n",
    "    \n",
    "    # training LGBM-1 with best parameters identified\n",
    "    lgb1 = LGBMClassifier(max_depth=best_param_1['max_depth'],subsample=best_param_1['subsample'],\n",
    "                     learning_rate=best_param_1['learning_rate'],n_estimators=best_param_1['n_estimators'],\n",
    "                     num_leaves=best_param_1['num_leaves'],colsample_bytree=best_param_1['colsample_bytree'],n_jobs=-1)\n",
    "\n",
    "    lgb1.fit(x_train,y_train)\n",
    "\n",
    "    pred_train_1 = lgb1.predict(x_train)\n",
    "    pred_test_1 = lgb1.predict(x_test)\n",
    "    print(\"Train accuracy of LGBM-1 = \",accuracy_score(y_train,pred_train_1))\n",
    "    print(\"Test accuracy of LGBM-1 = \",accuracy_score(y_test,pred_test_1))\n",
    "\n",
    "    pred_train_prob_1 = lgb1.predict_proba(x_train)\n",
    "    pred_test_prob_1 = lgb1.predict_proba(x_test)\n",
    "    print(\"Train loss of LGBM-1 = \",log_loss(y_train,pred_train_prob_1))\n",
    "    print(\"Test loss of LGBM-1 = \",log_loss(y_test,pred_test_prob_1))\n",
    "    print(\"Time taken to train LightGBM Model-1 = \", (datetime.now() - start_1))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    #============================================================================================================\n",
    "    \n",
    "    # training 2nd LightGBM\n",
    "    \n",
    "    print(\"LightGBM 2\")\n",
    "\n",
    "    start_2 = datetime.now()\n",
    "    \n",
    "    best_param_2 = get_best_parameters(x_train,y_train)\n",
    "\n",
    "    print(\"LGBMClassifier-2 best parameters:\", best_param_2)\n",
    "    \n",
    "    # training LGBM-2 with best parameters identified\n",
    "    lgb2 = LGBMClassifier(max_depth=best_param_2['max_depth'],subsample=best_param_2['subsample'],\n",
    "                     learning_rate=best_param_2['learning_rate'],n_estimators=best_param_2['n_estimators'],\n",
    "                     num_leaves=best_param_2['num_leaves'],colsample_bytree=best_param_2['colsample_bytree'],n_jobs=-1)\n",
    "\n",
    "    lgb2.fit(x_train,y_train)\n",
    "\n",
    "    pred_train_2 = lgb2.predict(x_train)\n",
    "    pred_test_2 = lgb2.predict(x_test)\n",
    "    print(\"Train accuracy of LGBM-2 = \",accuracy_score(y_train,pred_train_2))\n",
    "    print(\"Test accuracy of LGBM-2 = \",accuracy_score(y_test,pred_test_2))\n",
    "\n",
    "    pred_train_prob_2 = lgb2.predict_proba(x_train)\n",
    "    pred_test_prob_2 = lgb2.predict_proba(x_test)\n",
    "    print(\"Train loss of LGBM-2 = \",log_loss(y_train,pred_train_prob_2))\n",
    "    print(\"Test loss of LGBM-2 = \",log_loss(y_test,pred_test_prob_2))\n",
    "    print(\"Time taken to train LightGBM Model-2 = \", (datetime.now() - start_2))\n",
    "    print(\"=\"*50)\n",
    "#================================================================================================\n",
    "\n",
    "    # training 3rd LightGBM\n",
    "    \n",
    "    print(\"LightGBM 3\")\n",
    "\n",
    "    start_3 = datetime.now()\n",
    "    \n",
    "    best_param_3 = get_best_parameters(x_train,y_train)\n",
    "\n",
    "    print(\"LGBMClassifier-3 best parameters:\", best_param_3)\n",
    "\n",
    "    # training LGBM-3 with best parameters identified\n",
    "    lgb3 = LGBMClassifier(max_depth=best_param_3['max_depth'],subsample=best_param_3['subsample'],\n",
    "                     learning_rate=best_param_3['learning_rate'],n_estimators=best_param_3['n_estimators'],\n",
    "                     num_leaves=best_param_3['num_leaves'],colsample_bytree=best_param_3['colsample_bytree'],n_jobs=-1)\n",
    "\n",
    "    lgb3.fit(x_train,y_train)\n",
    "\n",
    "    pred_train_3 = lgb3.predict(x_train)\n",
    "    pred_test_3 = lgb3.predict(x_test)\n",
    "    print(\"Train accuracy of LGBM-3 = \",accuracy_score(y_train,pred_train_3))\n",
    "    print(\"Test accuracy of LGBM-3 = \",accuracy_score(y_test,pred_test_3))\n",
    "\n",
    "    pred_train_prob_3 = lgb3.predict_proba(x_train)\n",
    "    pred_test_prob_3 = lgb3.predict_proba(x_test)\n",
    "    print(\"Train loss of LGBM-3 = \",log_loss(y_train,pred_train_prob_3))\n",
    "    print(\"Test loss of LGBM-3 = \",log_loss(y_test,pred_test_prob_3))\n",
    "    print(\"Time taken to train LightGBM Model-3 = \", (datetime.now() - start_3))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    #=================================================================================================================\n",
    "    \n",
    "    # preparing train data for Feed Forward Neural Network (FFNN)\n",
    "    \n",
    "    # combining the predicted probababilities of three LGBM models for trainig FFNN\n",
    "\n",
    "    pred_train = np.hstack([pred_train_prob_1,pred_train_prob_2,pred_train_prob_3])\n",
    "    pred_test = np.hstack([pred_test_prob_1,pred_test_prob_2,pred_test_prob_3])\n",
    "\n",
    "    print(\"Shape of predicted train probabilities\",pred_train.shape)\n",
    "    print(\"Shape of predicted test probabilities\",pred_test.shape)\n",
    "    \n",
    "    # one-hot encoding class label for Neural Network \n",
    "    ytrain_ohe = onehot(y_train)\n",
    "    ytest_ohe = onehot(y_test)\n",
    "    \n",
    "    # defining architecture for FFNN\n",
    "    # Neural network on the probabilities of LGBM Classifiers\n",
    "\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(24,activation='sigmoid',input_shape=(6,)))\n",
    "    model_nn.add(Dropout(0.7))\n",
    "    model_nn.add(Dense(12,activation='sigmoid'))\n",
    "    model_nn.add(Dropout(0.7))\n",
    "    model_nn.add(Dense(6,activation='sigmoid'))\n",
    "    model_nn.add(Dropout(0.3))\n",
    "    model_nn.add(Dense(2,activation='softmax'))\n",
    "    opt = tensorflow.keras.optimizers.Adam(0.0001)\n",
    "    model_nn.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    #checkpoint to save best weights of model\n",
    "    checkpt = ModelCheckpoint(\"best_weights.h5\",monitor='val_loss', save_best_only=True,\n",
    "                          save_weights_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "    model_nn.fit(pred_train,ytrain_ohe,batch_size=256, epochs=25,validation_data=(pred_test,ytest_ohe),callbacks=[checkpt],verbose=2)\n",
    "    \n",
    "    model_nn.load_weights(\"best_weights.h5\")\n",
    "    \n",
    "    # predicting on train data\n",
    "    train_pred_prob_nn = model_nn.predict(pred_train)\n",
    "    train_pred_nn = [np.argmax(train_pred_prob_nn[i]) for i in range(pred_train.shape[0])]\n",
    "    print(\"Final accuracy on train data = \", accuracy_score(y_train,train_pred_nn))\n",
    "    print(\"Final loss on train data = \", log_loss(y_train,train_pred_prob_nn))\n",
    "    \n",
    "    # predicting on test data\n",
    "    test_pred_prob_nn = model_nn.predict(pred_test)\n",
    "    test_pred_nn = [np.argmax(test_pred_prob_nn[i]) for i in range(pred_test.shape[0])]\n",
    "    print(\"Final accuracy on test data = \", accuracy_score(y_test,test_pred_nn))\n",
    "    print(\"Final loss on test data = \", log_loss(y_test,test_pred_prob_nn))\n",
    "    \n",
    "    print(\"Model training completed\")\n",
    "    \n",
    "    #=============================================================================================\n",
    "    \n",
    "    # saving model and weights for predictions\n",
    "    \n",
    "    print(\"Please wait...saving trained model...\")\n",
    "    \n",
    "    with open(\"median_data.pickle\",'wb') as file:\n",
    "        pickle.dump(train_data_median,file)\n",
    "    \n",
    "    with open(\"mode_data.pickle\",'wb') as file:\n",
    "        pickle.dump(train_data_mode,file)\n",
    "        \n",
    "    with open(\"LightGBM_1.pickle\",'wb') as file:\n",
    "        pickle.dump(lgb1,file)\n",
    "        \n",
    "    with open(\"LightGBM_2.pickle\",'wb') as file:\n",
    "        pickle.dump(lgb2,file)\n",
    "        \n",
    "    with open(\"LightGBM_3.pickle\",'wb') as file:\n",
    "        pickle.dump(lgb3,file)\n",
    "        \n",
    "    model_json = model_nn.to_json()\n",
    "    \n",
    "    with open(\"model_ffnn.json\",\"w\") as file:\n",
    "        file.write(model_json)\n",
    "    \n",
    "    print(\"Models saved.\")\n",
    "    print(\"DONE\")\n",
    "    #============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging all dataframes =  (970960, 26)\n",
      "Shape after feature engineering =  (970960, 42)\n",
      "Shape after splitting:\n",
      "Train shape: x_train =  (728220, 38) , y_train =  (728220,)\n",
      "Test shape: x_test =  (242740, 38) , y_test =  (242740,)\n",
      "LightGBM 1\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 19.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier-1 best parameters: {'subsample': 0.5, 'num_leaves': 100, 'n_estimators': 200, 'max_depth': 13, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Train accuracy of LGBM-1 =  0.9625236878965148\n",
      "Test accuracy of LGBM-1 =  0.9613125154486282\n",
      "Train loss of LGBM-1 =  0.10925273771694329\n",
      "Test loss of LGBM-1 =  0.11650275727421662\n",
      "Time taken to train LightGBM Model-1 =  0:20:52.621027\n",
      "==================================================\n",
      "LightGBM 2\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 23.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier-2 best parameters: {'subsample': 0.5, 'num_leaves': 80, 'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n",
      "Train accuracy of LGBM-2 =  0.9656903133668396\n",
      "Test accuracy of LGBM-2 =  0.9610859355689215\n",
      "Train loss of LGBM-2 =  0.09871427276954055\n",
      "Test loss of LGBM-2 =  0.11697049258910246\n",
      "Time taken to train LightGBM Model-2 =  0:24:45.572408\n",
      "==================================================\n",
      "LightGBM 3\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 16.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier-3 best parameters: {'subsample': 0.5, 'num_leaves': 35, 'n_estimators': 500, 'max_depth': 11, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "Train accuracy of LGBM-3 =  0.9618892642333361\n",
      "Test accuracy of LGBM-3 =  0.9607522452006262\n",
      "Train loss of LGBM-3 =  0.1113491552745651\n",
      "Test loss of LGBM-3 =  0.11776111488662007\n",
      "Time taken to train LightGBM Model-3 =  0:18:08.838687\n",
      "==================================================\n",
      "Shape of predicted train probabilities (728220, 6)\n",
      "Shape of predicted test probabilities (242740, 6)\n",
      "WARNING:tensorflow:From C:\\Users\\JAYANY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 728220 samples, validate on 242740 samples\n",
      "WARNING:tensorflow:From C:\\Users\\JAYANY\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/25\n",
      "728220/728220 - 13s - loss: 0.3462 - acc: 0.9100 - val_loss: 0.2939 - val_acc: 0.9101\n",
      "Epoch 2/25\n",
      "728220/728220 - 12s - loss: 0.3039 - acc: 0.9101 - val_loss: 0.2604 - val_acc: 0.9101\n",
      "Epoch 3/25\n",
      "728220/728220 - 12s - loss: 0.2689 - acc: 0.9101 - val_loss: 0.2057 - val_acc: 0.9101\n",
      "Epoch 4/25\n",
      "728220/728220 - 12s - loss: 0.2338 - acc: 0.9101 - val_loss: 0.1739 - val_acc: 0.9101\n",
      "Epoch 5/25\n",
      "728220/728220 - 12s - loss: 0.2152 - acc: 0.9158 - val_loss: 0.1613 - val_acc: 0.9101\n",
      "Epoch 6/25\n",
      "728220/728220 - 11s - loss: 0.2046 - acc: 0.9326 - val_loss: 0.1561 - val_acc: 0.9611\n",
      "Epoch 7/25\n",
      "728220/728220 - 4s - loss: 0.1992 - acc: 0.9392 - val_loss: 0.1531 - val_acc: 0.9608\n",
      "Epoch 8/25\n",
      "728220/728220 - 4s - loss: 0.1957 - acc: 0.9451 - val_loss: 0.1513 - val_acc: 0.9606\n",
      "Epoch 9/25\n",
      "728220/728220 - 4s - loss: 0.1930 - acc: 0.9474 - val_loss: 0.1498 - val_acc: 0.9604\n",
      "Epoch 10/25\n",
      "728220/728220 - 4s - loss: 0.1910 - acc: 0.9476 - val_loss: 0.1487 - val_acc: 0.9602\n",
      "Epoch 11/25\n",
      "728220/728220 - 4s - loss: 0.1886 - acc: 0.9478 - val_loss: 0.1476 - val_acc: 0.9600\n",
      "Epoch 12/25\n",
      "728220/728220 - 4s - loss: 0.1871 - acc: 0.9483 - val_loss: 0.1469 - val_acc: 0.9598\n",
      "Epoch 13/25\n",
      "728220/728220 - 4s - loss: 0.1868 - acc: 0.9478 - val_loss: 0.1460 - val_acc: 0.9596\n",
      "Epoch 14/25\n",
      "728220/728220 - 4s - loss: 0.1860 - acc: 0.9479 - val_loss: 0.1456 - val_acc: 0.9595\n",
      "Epoch 15/25\n",
      "728220/728220 - 4s - loss: 0.1845 - acc: 0.9483 - val_loss: 0.1453 - val_acc: 0.9593\n",
      "Epoch 16/25\n",
      "728220/728220 - 4s - loss: 0.1843 - acc: 0.9475 - val_loss: 0.1448 - val_acc: 0.9592\n",
      "Epoch 17/25\n",
      "728220/728220 - 4s - loss: 0.1834 - acc: 0.9479 - val_loss: 0.1444 - val_acc: 0.9591\n",
      "Epoch 18/25\n",
      "728220/728220 - 4s - loss: 0.1824 - acc: 0.9480 - val_loss: 0.1443 - val_acc: 0.9590\n",
      "Epoch 19/25\n",
      "728220/728220 - 4s - loss: 0.1817 - acc: 0.9479 - val_loss: 0.1444 - val_acc: 0.9590\n",
      "Epoch 20/25\n",
      "728220/728220 - 4s - loss: 0.1812 - acc: 0.9480 - val_loss: 0.1439 - val_acc: 0.9589\n",
      "Epoch 21/25\n",
      "728220/728220 - 4s - loss: 0.1806 - acc: 0.9478 - val_loss: 0.1436 - val_acc: 0.9588\n",
      "Epoch 22/25\n",
      "728220/728220 - 4s - loss: 0.1806 - acc: 0.9475 - val_loss: 0.1437 - val_acc: 0.9588\n",
      "Epoch 23/25\n",
      "728220/728220 - 4s - loss: 0.1797 - acc: 0.9478 - val_loss: 0.1434 - val_acc: 0.9586\n",
      "Epoch 24/25\n",
      "728220/728220 - 4s - loss: 0.1792 - acc: 0.9478 - val_loss: 0.1436 - val_acc: 0.9586\n",
      "Epoch 25/25\n",
      "728220/728220 - 4s - loss: 0.1795 - acc: 0.9475 - val_loss: 0.1430 - val_acc: 0.9585\n",
      "Final accuracy on train data =  0.9615953969954135\n",
      "Final loss on train data =  0.12798558805274754\n",
      "Final accuracy on test data =  0.9584864464035594\n",
      "Final loss on test data =  0.14303264218579975\n",
      "Model training completed\n",
      "Please wait...saving trained model...\n",
      "Models saved.\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def churn_predictor():\n",
    "    \n",
    "    '''\n",
    "    function to predict customer churn \n",
    "    '''\n",
    "        \n",
    "    with open(\"median_data.pickle\",'rb') as file:\n",
    "        train_data_median = pickle.load(file)\n",
    "        \n",
    "    with open(\"mode_data.pickle\",'rb') as file:\n",
    "        train_data_mode = pickle.load(file)\n",
    "        \n",
    "    with open(\"LightGBM_1.pickle\",'rb') as file:\n",
    "        lgb1 = pickle.load(file)\n",
    "        \n",
    "    with open(\"LightGBM_2.pickle\",'rb') as file:\n",
    "        lgb2 = pickle.load(file)\n",
    "        \n",
    "    with open(\"LightGBM_3.pickle\",'rb') as file:\n",
    "        lgb3 = pickle.load(file)\n",
    "        \n",
    "    with open(\"model_ffnn.json\",\"r\") as file:\n",
    "        model_json = file.read()\n",
    "        \n",
    "    model_nn = tensorflow.keras.models.model_from_json(model_json)\n",
    "    model_nn.load_weights(\"best_weights.h5\")\n",
    "        \n",
    "    sample_submission_v2 = pd.read_csv(\"input_files/sample_submission_v2.csv\",dtype={'is_churn':np.int8})\n",
    "    \n",
    "    transactions_v2 = pd.read_csv(\"input_files/transactions_v2.csv\",\n",
    "                              dtype={'payment_method_id':np.int8,'payment_plan_days':np.int16,'plan_list_price':np.int16,\n",
    "                                    'actual_amount_paid':np.int16,'is_auto_renew':np.int8,'is_cancel':np.int8})\n",
    "    members = pd.read_csv(\"input_files/members_v3.csv\",\n",
    "                     dtype={'city':np.int8,'bd':np.int16,'registered_via':np.int8})\n",
    "    user_logs_v2 = pd.read_csv(\"input_files/user_logs_v2.csv\",\n",
    "                     dtype={'num_25': np.int16,'num_50': np.int16,'num_75': np.int16,'num_985': np.int16,'num_100': np.int16})\n",
    "    \n",
    "        \n",
    "    # taking transaction count per user\n",
    "    transaction_count_per_user = transactions_v2.groupby(\"msno\")['msno'].count()\n",
    "    \n",
    "    # converting transaction count to dataframe\n",
    "    transaction_count_per_user = pd.DataFrame(transaction_count_per_user)\n",
    "    transaction_count_per_user.columns = ['trans_count']\n",
    "    transaction_count_per_user.reset_index(inplace=True)\n",
    "    \n",
    "    #keeping only latest transaction record for each user\n",
    "    transactions_v2 = transactions_v2.drop_duplicates(subset=['msno'], keep='first')\n",
    "    #dropping columns\n",
    "    transactions_v2.drop(columns=['membership_expire_date','actual_amount_paid','is_cancel'],axis=1,inplace=True)\n",
    "    #merging transaction count to dataframe\n",
    "    transactions_v2 = transactions_v2.merge(transaction_count_per_user, how='left',on='msno')\n",
    "    \n",
    "    user_logs_v2.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # removing datapoints with negative value of num_100\n",
    "    user_logs_v2.drop(index=(user_logs_v2[user_logs_v2.num_100<0].index) ,inplace=True)\n",
    "    \n",
    "    # converting total seconds to total minutes by dividing it by 60\n",
    "    user_logs_v2['total_minutes'] = user_logs_v2['total_secs']/60\n",
    "    user_logs_v2.drop(columns=['total_secs'],inplace=True)  # dropping column 'total_secs'\n",
    "    \n",
    "    # taking sum of each columns for each user\n",
    "    user_logs_sum = user_logs_v2.groupby('msno',as_index=False).sum()\n",
    "    user_logs_sum.drop(columns=['date'],axis=1,inplace=True)\n",
    "    \n",
    "    # taking mean of each columns for each user\n",
    "    user_logs_mean = user_logs_v2.groupby('msno',as_index=False).mean()\n",
    "    user_logs_mean.columns = ['msno','date','mean_num_25','mean_num_50','mean_num_75','mean_num_985','mean_num_100','mean_num_unq','mean_total_min']\n",
    "    user_logs_mean.drop(columns=['date'],axis=1,inplace=True)\n",
    "    \n",
    "    # merging dataframe\n",
    "    user_logs = user_logs_sum.merge(user_logs_mean,on='msno')\n",
    "    #user_logs = user_logs.merge(user_logs_std,on='msno')\n",
    "    \n",
    "    members.drop_duplicates(inplace=True)\n",
    "    members.drop(columns=['gender'],axis=1,inplace=True)  # dropping 'gender' column as 65% values are NULL\n",
    "    \n",
    "    # merging test labels with transactions dataframe\n",
    "    test = sample_submission_v2.merge(transactions_v2,how='left',on='msno')\n",
    "\n",
    "    # merging members data for test dataset users\n",
    "    test = test.merge(members,how='left',on='msno')\n",
    "\n",
    "    # merging test user's logs \n",
    "    test = test.merge(user_logs, how='left',on='msno')\n",
    "    \n",
    "    numerical_columns = ['payment_plan_days','plan_list_price','num_25', 'num_50', 'num_75', 'num_985', 'num_100',\n",
    "                     'num_unq', 'total_minutes', 'mean_num_25', 'mean_num_50', 'mean_num_75','mean_num_985', \n",
    "                     'mean_num_100', 'mean_num_unq', 'mean_total_min','bd','trans_count']\n",
    "    \n",
    "    categorical_columns = ['payment_method_id','is_auto_renew','city','registered_via','transaction_date','registration_init_time']\n",
    "    \n",
    "\n",
    "    #filling categorical fetaures like city,registered_via etc, with mode values i.e. most occured value for that feature\n",
    "    fill_with_mode(test,categorical_columns,train_data_mode)\n",
    "\n",
    "    #filling numerical fetaures with median values \n",
    "    fill_with_median(test,numerical_columns,train_data_median)\n",
    "\n",
    "    test.drop(columns=['bd'],axis=1,inplace=True)  # drop 'bd' (age) column\n",
    "    \n",
    "    # creating new features out of registration date \n",
    "\n",
    "    # taking just day of month\n",
    "    test['trans_day_of_month'] = test.transaction_date.apply(lambda x: np.int8(str(int(x))[6:]))\n",
    "\n",
    "    # taking removing year from the date and taking just month and day of month \n",
    "    test['trans_month_day'] = test.transaction_date.apply(lambda x: np.int16(str(int(x))[4:]))\n",
    "\n",
    "    # 'day_of_week' signifies which day of week it is. \n",
    "    # e.g.  Monday is 1 and Sunday is 7\n",
    "    test['trans_day_of_week'] = test.transaction_date.apply(lambda x: datetime.strptime(str(int(x)), \"%Y%m%d\").isoweekday())\n",
    "\n",
    "    #=======================================================================================================\n",
    "\n",
    "    # creating new features out of registration date \n",
    "\n",
    "    # taking just day of month\n",
    "    test['reg_day_of_month'] = test.registration_init_time.apply(lambda x: np.int8(str(int(x))[6:]))\n",
    "\n",
    "    # taking just month of transaction\n",
    "    test['reg_month'] = test.registration_init_time.apply(lambda x: np.int8(str(int(x))[4:6]) )\n",
    "\n",
    "    # taking removing year from the date and taking just month and day of month \n",
    "    test['reg_month_day'] = test.registration_init_time.apply(lambda x: np.int16(str(int(x))[4:]))\n",
    "\n",
    "    # 'day_of_week' signifies which day of week it is. \n",
    "    # e.g.  Monday is 1 and Sunday is 7\n",
    "    test['reg_day_of_week'] = test.registration_init_time.apply(lambda x: datetime.strptime(str(int(x)), \"%Y%m%d\").isoweekday())\n",
    "\n",
    "    #=======================================================================================================\n",
    "\n",
    "    # creating a new feature - price_per_day\n",
    "\n",
    "    test['price_per_day'] = [ (test['plan_list_price'][i]/test['payment_plan_days'][i]) if (test['payment_plan_days'][i]!=0) else 0 for i in range(test.shape[0]) ]\n",
    "\n",
    "    #=======================================================================================================\n",
    "\n",
    "    # creating a new feature - min_per_song i.e. average minutes listened per song\n",
    "\n",
    "    test['min_per_song'] = test['total_minutes']/(test['num_25']+test['num_50']+test['num_75']+test['num_985']+test['num_100'])\n",
    "\n",
    "    # creating a new feature - avg_min_per_unq i.e. average minutes listened per unique song\n",
    "\n",
    "    test['avg_min_per_unq'] = test['mean_total_min']/test['mean_num_unq']\n",
    "\n",
    "    #========================================================================================================\n",
    "\n",
    "    # creating a feature 'total_num' i.e. total number of songs listened by the user\n",
    "    test['total_num'] = test['num_25']+test['num_50']+test['num_75']+test['num_985']+test['num_100']\n",
    "\n",
    "    # creating a feature 'ratio_num_25' i.e. ratio of 'total_num'(total number of songs) and 'num_25'(number of songs listened upto 25% of their length)\n",
    "    # train['ratio_num_25'] = [train['total_num'][i]/train['num_25'][i] for i in range(train.shape[0])]\n",
    "    test['ratio_num_25'] = test['num_25']/test['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_50' i.e. ratio of 'total_num'(total number of songs) and 'num_50'(number of songs listened upto 50% of their length)\n",
    "    test['ratio_num_50'] = test['num_50']/test['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_75' i.e. ratio of 'total_num'(total number of songs) and 'num_75'(number of songs listened upto 75% of their length)\n",
    "    test['ratio_num_75'] = test['num_75']/test['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_985' i.e. ratio of 'total_num'(total number of songs) and 'num_985'(number of songs listened upto 98.5% of their length)\n",
    "    test['ratio_num_985'] = test['num_985']/test['total_num']\n",
    "\n",
    "    # creating a feature 'ratio_num_100' i.e. ratio of 'total_num'(total number of songs) and 'num_100'(number of songs listened upto 100% of their length)\n",
    "    test['ratio_num_100'] = test['num_100']/test['total_num']\n",
    "\n",
    "    # creating a feature 'total_by_unq' i.e. total_num divided by num_unq\n",
    "    test['total_by_unq'] = test['total_num']/test['num_unq']\n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    # test data provided by kaggle\n",
    "    test_x = test.drop(columns=['msno','is_churn','transaction_date','registration_init_time'],axis=1)\n",
    "\n",
    "    test_y = test['is_churn'].copy()\n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    # getting accuracy and loss on test data provided by kaggle\n",
    "    test_pred1 = lgb1.predict(test_x)\n",
    "\n",
    "    test_pred_prob1 = lgb1.predict_proba(test_x)\n",
    "    #####################################################################\n",
    "\n",
    "    # getting accuracy and loss on test data provided by kaggle\n",
    "    test_pred2 = lgb2.predict(test_x)\n",
    "    test_pred_prob2 = lgb2.predict_proba(test_x)\n",
    "    \n",
    "    ######################################################################\n",
    "\n",
    "    # getting accuracy and loss on test data provided by kaggle\n",
    "    test_pred3 = lgb3.predict(test_x)\n",
    "    test_pred_prob3 = lgb3.predict_proba(test_x)\n",
    "    \n",
    "    \n",
    "    # concatenating predicted probabilities by 3 LGBM Classifiers for input to Neural Network \n",
    "    test_x_nn = np.hstack([test_pred_prob1,test_pred_prob2,test_pred_prob3])\n",
    "    \n",
    "    # one-hot encoding class-label \n",
    "    test_y_ohe = onehot(test_y)\n",
    "    \n",
    "    # predicting is_churn\n",
    "    test_pred_prob_nn = model_nn.predict(test_x_nn)\n",
    "    test_pred_nn = [np.argmax(test_pred_prob_nn[i]) for i in range(test_x_nn.shape[0])]\n",
    "\n",
    "    print(\"Accuracy = \", accuracy_score(test_y,test_pred_nn))\n",
    "    print(\"Loss = \", log_loss(test_y,test_pred_prob_nn,labels=[0,1]))\n",
    "    \n",
    "    # taking out probabilities of class '1' i.e. churn\n",
    "    churn_prob = [p[1] for p in test_pred_prob_nn]\n",
    "    \n",
    "    # saving predicted probabilities in a csv file\n",
    "    submission = pd.DataFrame()\n",
    "    submission['msno'] = test['msno']\n",
    "    submission['is_churn'] = churn_prob\n",
    "    #print(submission_7.shape)\n",
    "    submission.to_csv(\"submission_test.csv\",index=False)\n",
    "    submission['is_churn_class'] = test_pred_nn\n",
    "    submission.to_csv(\"submission_test_class.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Accuracy =  0.9501328417106442\n",
      "Loss =  0.06224723566259524\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#predicting on test data provided by Kaggle\n",
    "churn_predictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
